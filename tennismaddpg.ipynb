{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c50e4b",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep Reinforcement Learning: Cooperative Tennis\n",
    "\n",
    "This notebook implements a Multi-Agent Deep Deterministic Policy Gradient (MADDPG) system to train two cooperative agents to play tennis in the Unity ML-Agents environment.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "The goal is to train two agents controlling rackets to keep a ball in play for as long as possible. Each agent receives a reward of +0.1 for successfully hitting the ball over the net, and -0.01 for letting it hit the ground or go out of bounds.\n",
    "\n",
    "<table style=\"margin: auto;\">\n",
    "  <tr>\n",
    "      <td>Trained Agents in Action</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"animation.gif\" align=\"center\" width=\"500\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Algorithm**: DDPG with distributional learning, noisy layers, and prioritized experience replay\n",
    "- **Environment**: Unity ML-Agents Tennis (continuous control)\n",
    "- **State Space**: 24 dimensions per agent (position & velocity of racket and ball)\n",
    "- **Action Space**: 2 continuous actions per agent (movement toward/away from net, jumping)\n",
    "- **Success Criterion**: Average score â‰¥ 0.5 over 100 consecutive episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3df71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from ddpg_wrapper import Agent\n",
    "from wrapper import ddpg\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.empty(1, device=device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd173d",
   "metadata": {},
   "source": [
    "## Environment Setup and Initialisation\n",
    "\n",
    "First, I'll load the Unity Tennis environment and examine its properties. The environment consists of two agents that must learn to cooperate in order to achieve the longest possible rally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98014a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Unity Tennis environment\n",
    "# Use project-relative path so no absolute user path is exposed\n",
    "tennis_env = UnityEnvironment(\n",
    "    file_name=\"Unity/Tennis.app\" #Or any path you choose\n",
    ")\n",
    "\n",
    "# Reset and get initial state\n",
    "tennis_env.reset()\n",
    "\n",
    "# Get behavior name\n",
    "behavior_names = list(tennis_env.behavior_specs)\n",
    "brain_id = behavior_names[0]\n",
    "spec = tennis_env.behavior_specs[brain_id]\n",
    "\n",
    "# Get environment info\n",
    "decision_steps, terminal_steps = tennis_env.get_steps(brain_id)\n",
    "n_agents = len(decision_steps)\n",
    "action_dim = spec.action_spec.continuous_size\n",
    "state_dim = spec.observation_specs[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display environment specifications\n",
    "print(f'Number of cooperative agents: {n_agents}\\n')\n",
    "print(f'Dimension of action space: {action_dim}\\n')\n",
    "\n",
    "# Examine observation space\n",
    "current_states = env_state.vector_observations\n",
    "print(f'Each of the {current_states.shape[0]} agents observes a state vector of length: {state_dim}')\n",
    "print(f'Initial state for agent 0:\\n{current_states[0]}\\n')\n",
    "\n",
    "# Verify environment structure\n",
    "print('\\nEnvironment Structure:')\n",
    "print(f\"  Reward signals: {len(env_state.rewards)}\")\n",
    "print(f\"  Observation tensor shape: {env_state.vector_observations.shape}\")\n",
    "print(f\"  Done flags: {len(env_state.local_done)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test environment with random actions\n",
    "num_test_episodes = 5\n",
    "\n",
    "for episode_idx in range(num_test_episodes):\n",
    "    env_state = tennis_env.reset(train_mode=False)[brain_id]\n",
    "    current_states = env_state.vector_observations\n",
    "    episode_rewards = np.zeros(n_agents)\n",
    "    timesteps = 0\n",
    "    \n",
    "    while True:\n",
    "        timesteps += 1\n",
    "        # Sample random actions from uniform distribution\n",
    "        random_actions = np.random.randn(n_agents, action_dim)\n",
    "        random_actions = np.clip(random_actions, -1, 1)\n",
    "        \n",
    "        env_state = tennis_env.step(random_actions)[brain_id]\n",
    "        next_states = env_state.vector_observations\n",
    "        rewards = env_state.rewards\n",
    "        dones = env_state.local_done\n",
    "        \n",
    "        episode_rewards += rewards\n",
    "        current_states = next_states\n",
    "        \n",
    "        if np.any(dones):\n",
    "            break\n",
    "    \n",
    "    print(f'Episode {episode_idx + 1} | Steps: {timesteps} | Max score: {np.max(episode_rewards):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the DDPG agent with advanced features\n",
    "agent_config = {\n",
    "    # Reproducibility\n",
    "    'seed': 42,\n",
    "    \n",
    "    # Core learning parameters\n",
    "    'batch_size': 1024,\n",
    "    'buffer_size': int(1e6),\n",
    "    'start_since': 1024,\n",
    "    'gamma': 0.95,\n",
    "    'update_every': 1,\n",
    "    'n_updates': 1,\n",
    "    'tau': 0.2,\n",
    "    \n",
    "    # Network optimization\n",
    "    'actor_lr': 1e-3,\n",
    "    'critic_lr': 1e-3,\n",
    "    'clip': None,\n",
    "    'weight_decay': 0,\n",
    "    \n",
    "    # Advanced features\n",
    "    'distributional': True,\n",
    "    \n",
    "    # Prioritized Experience Replay\n",
    "    'priority_eps': 1e-3,\n",
    "    'a': 0.,\n",
    "    \n",
    "    # Multi-step returns\n",
    "    'n_multisteps': 1,\n",
    "    \n",
    "    # Distributional RL support values\n",
    "    'v_min': -0.1,\n",
    "    'v_max': 0.1,\n",
    "    'n_atoms': 51,\n",
    "    \n",
    "    # Noisy networks for exploration\n",
    "    'initial_sigma': 0.500,\n",
    "    'linear_type': 'noisy',\n",
    "    'factorized': True\n",
    "}\n",
    "\n",
    "# Training configuration\n",
    "training_config = {\n",
    "    'n_episodes': 500,  # Train for longer to ensure robust convergence\n",
    "    'continue_after_solved': True,\n",
    "    \n",
    "    # Exploration schedule (using noisy layers, so OU noise disabled)\n",
    "    'eps_start': 0.,\n",
    "    'eps_min': 0.,\n",
    "    'eps_decay': 0.,\n",
    "    \n",
    "    # Importance sampling for prioritized replay\n",
    "    'beta_start': 0.,\n",
    "    'beta_end': 0.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c39be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DDPG agent\n",
    "tennis_agent = Agent(state_dim, action_dim, n_agents, **agent_config)\n",
    "\n",
    "# Display network architectures\n",
    "print(\"Actor Network Architecture:\")\n",
    "print(tennis_agent.actor_local)\n",
    "print(\"\\nCritic Network Architecture:\")\n",
    "print(tennis_agent.critic_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae28870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "training_scores = ddpg(tennis_env, tennis_agent, **training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training performance with custom styling\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs('results', exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5), facecolor='#2E3440')\n",
    "ax = fig.add_subplot(111)\n",
    "ax.grid(True, alpha=0.3, linestyle='--', color='#4C566A')\n",
    "ax.set_facecolor('#3B4252')\n",
    "\n",
    "# Plot raw scores\n",
    "ax.plot(np.arange(len(training_scores)), training_scores,\n",
    "        alpha=0.6, linewidth=1.5, color='#88C0D0', label=\"Episode Score\")\n",
    "\n",
    "# Plot moving average\n",
    "moving_avg = np.array([np.mean(training_scores[max(0, i-100):i]) \n",
    "                       for i in range(1, len(training_scores) + 1)])\n",
    "ax.plot(np.arange(len(training_scores)), moving_avg,\n",
    "        alpha=0.9, linewidth=2.5, color='#A3BE8C', label=\"100-Episode Average\")\n",
    "\n",
    "# Mark solved threshold\n",
    "ax.axhline(y=0.5, color='#BF616A', linestyle='--', linewidth=2, label='Solved Threshold')\n",
    "\n",
    "# Styling\n",
    "ax.legend(loc='upper left', framealpha=0.9)\n",
    "ax.set_ylabel('Max Score per Episode', color='#ECEFF4', fontsize=12)\n",
    "ax.set_xlabel('Episode Number', color='#ECEFF4', fontsize=12)\n",
    "ax.set_title('MADDPG Training Progress', color='#ECEFF4', fontsize=14, fontweight='bold')\n",
    "ax.tick_params(colors='#ECEFF4')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot to results folder\n",
    "plot_path = os.path.join('results', f'training_plot_{timestamp}.png')\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='#2E3440')\n",
    "print(f'Training plot saved to {plot_path}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model and training artifacts\n",
    "torch.save(\n",
    "    f='trained_tennis_agent.pth',\n",
    "    obj={\n",
    "        'agent_config': agent_config,\n",
    "        'training_config': training_config,\n",
    "        'episode_scores': training_scores,\n",
    "        'actor_weights': tennis_agent.actor_local.state_dict(),\n",
    "        'critic_weights': tennis_agent.critic_local.state_dict()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcccd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously trained agent\n",
    "checkpoint = torch.load(\n",
    "    f=\"trained_tennis_agent.pth\",\n",
    "    map_location=device\n",
    ")\n",
    "\n",
    "tennis_agent = Agent(state_dim, action_dim, n_agents, **checkpoint['agent_config'])\n",
    "tennis_agent.actor_local.load_state_dict(checkpoint['actor_weights'], strict=False)\n",
    "tennis_agent.critic_local.load_state_dict(checkpoint['critic_weights'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f856ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the trained agent play\n",
    "env_state = tennis_env.reset(train_mode=False)[brain_id]\n",
    "current_states = env_state.vector_observations\n",
    "episode_rewards = np.zeros(n_agents)\n",
    "step_counter = 0\n",
    "\n",
    "while True:\n",
    "    actions = tennis_agent.act(current_states)\n",
    "    env_state = tennis_env.step(actions)[brain_id]\n",
    "    next_states = env_state.vector_observations\n",
    "    rewards = env_state.rewards\n",
    "    dones = env_state.local_done\n",
    "    episode_rewards += rewards\n",
    "    current_states = next_states\n",
    "    step_counter += 1\n",
    "    \n",
    "    print(f\"\\rTimestep {step_counter} | Current max score: {np.max(episode_rewards):.3f}\", end='')\n",
    "    \n",
    "    if np.any(dones):\n",
    "        break\n",
    "\n",
    "print(f\"\\n\\nFinal episode score: {np.max(episode_rewards):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b95aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation over multiple episodes\n",
    "n_eval_episodes = 150  # Extended evaluation for statistical significance\n",
    "\n",
    "episode_scores = []\n",
    "status_format = \"\\rEpisode {}/{} | Current: {:.3f} | Previous: {:.3f} | Mean: {:.3f}\"\n",
    "\n",
    "for ep_num in range(n_eval_episodes):\n",
    "    env_state = tennis_env.reset(train_mode=True)[brain_id]\n",
    "    current_states = env_state.vector_observations\n",
    "    ep_rewards = np.zeros(n_agents)\n",
    "    \n",
    "    while True:\n",
    "        actions = tennis_agent.act(current_states)\n",
    "        env_state = tennis_env.step(actions)[brain_id]\n",
    "        next_states = env_state.vector_observations\n",
    "        rewards = env_state.rewards\n",
    "        dones = env_state.local_done\n",
    "        ep_rewards += rewards\n",
    "        current_states = next_states\n",
    "        \n",
    "        prev_score = episode_scores[-1] if episode_scores else 0\n",
    "        mean_score = np.mean(episode_scores) if episode_scores else 0\n",
    "        print(status_format.format(\n",
    "            ep_num + 1, n_eval_episodes,\n",
    "            np.max(ep_rewards), prev_score, mean_score\n",
    "        ), end='')\n",
    "        \n",
    "        if np.any(dones):\n",
    "            break\n",
    "    \n",
    "    episode_scores.append(np.max(ep_rewards))\n",
    "    \n",
    "    if (ep_num + 1) % 10 == 0:\n",
    "        print()\n",
    "\n",
    "print(f\"\\n\\nEvaluation complete!\")\n",
    "print(f\"Average score over {n_eval_episodes} episodes: {np.mean(episode_scores):.3f}\")\n",
    "print(f\"Standard deviation: {np.std(episode_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df85180",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Close the Unity environment to free system resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tennis_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
